@article{channarongHybridBERT4RecHybridContentBased2022,
  title = {{{HybridBERT4Rec}}: {{A Hybrid}} ({{Content-Based Filtering}} and {{Collaborative Filtering}}) {{Recommender System Based}} on {{BERT}}},
  shorttitle = {{{HybridBERT4Rec}}},
  author = {Channarong, Chanapa and Paosirikul, Chawisa and Maneeroj, Saranya and Takasu, Atsuhiro},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {56193--56206},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3177610},
  urldate = {2023-11-02},
  abstract = {Because a user's behavior depends mainly on the user's current interests, which can change over time, the sequential recommendation approach has become more prevalent in recommender systems. Many methods have been proposed that aim to model sequential recommendations. One of these is BERT4Rec, which applies the bidirectional-encoder-representations-from-transformers (BERT) technique to model user behavior sequences by considering the target user's historical data, i.e., a content-based filtering (CBF) approach. Despite BERT4Rec's effectiveness, we argue that considering only this historical data is insufficient to provide the most accurate recommendation. We believe that if BERT were to consider other users' interactions in its analysis, it would increase the model accuracy. Therefore, we propose a new method called HybridBERT4Rec, which applies BERT to both CBF and collaborative filtering (CF). For CBF, we want to extract the characteristics of the target user's interactions with purchased items. (We implement this in the same way as in BERT4Rec, with our model generating a target user profile.) For CF, we want to find neighboring users who are similar to the target user. Here, we extract the target item's characteristics using all other users who rated the target item as a second input to BERT. This generates a target item profile. After obtaining both profiles, we use them to predict a rating score. We experimented with three datasets, finding that our model was more accurate than the original BERT4Rec.},
  file = {/Users/I516998/Zotero/storage/5GK2YPY2/Channarong et al. - 2022 - HybridBERT4Rec A Hybrid (Content-Based Filtering .pdf}
}

@misc{heNeuralCollaborativeFiltering2017,
  title = {Neural {{Collaborative Filtering}}},
  author = {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
  year = {2017},
  month = aug,
  number = {arXiv:1708.05031},
  eprint = {1708.05031},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-02},
  abstract = {In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/Users/I516998/Zotero/storage/M5CVYY3A/He et al. - 2017 - Neural Collaborative Filtering.pdf;/Users/I516998/Zotero/storage/RB6NQHIA/1708.html}
}

@misc{hidasiSessionbasedRecommendationsRecurrent2016,
  title = {Session-Based {{Recommendations}} with {{Recurrent Neural Networks}}},
  author = {Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros and Baltrunas, Linas and Tikk, Domonkos},
  year = {2016},
  month = mar,
  number = {arXiv:1511.06939},
  eprint = {1511.06939},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.06939},
  urldate = {2024-01-09},
  abstract = {We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/I516998/Zotero/storage/IQK9MPWA/Hidasi et al. - 2016 - Session-based Recommendations with Recurrent Neura.pdf;/Users/I516998/Zotero/storage/M2BX4BDW/1511.html}
}

@misc{Ilias,
  title = {Ilias.De},
  urldate = {2024-01-05},
  howpublished = {https://www.ilias.de/},
  file = {/Users/I516998/Zotero/storage/ZFMC8KW8/www.ilias.de.html}
}

@article{jeevamolOntologybasedHybridElearning2021,
  title = {An Ontology-Based Hybrid e-Learning Content Recommender System for Alleviating the Cold-Start Problem},
  author = {Jeevamol, Joy and Renumol, V. G.},
  year = {2021},
  month = jul,
  journal = {Educ Inf Technol},
  volume = {26},
  number = {4},
  pages = {4993--5022},
  issn = {1573-7608},
  doi = {10.1007/s10639-021-10508-0},
  urldate = {2024-01-05},
  abstract = {An e-learning recommender system (RS) aims to generate personalized recommendations based on learner preferences and goals. The existing RSs in the e-learning domain still exhibit drawbacks due to its inability to consider the learner characteristics in the recommendation process. In this paper, we are dealing with the new user cold-start problem, which is a major drawback in e-learning content RSs. This problem can be mitigated by incorporating additional learner data in the recommendation process. This paper proposes an ontology-based (OB) content recommender system for addressing the new user cold-start problem. In the proposed recommendation model, ontology is used to model the learner and learning objects with their characteristics. Collaborative and content-based filtering techniques are used in the recommendation model to generate the top N recommendations based on learner ratings. Experiments were conducted to evaluate the performance and prediction accuracy of the proposed model in cold-start conditions using the evaluation metrics mean absolute error, precision and recall. The proposed model provides more reliable and personalized recommendations by making use of ontological domain knowledge.},
  langid = {english},
  keywords = {Cold-start problem,Content recommender systems,E-learning,Learning management system,Ontology,Personalized learning environment}
}

@misc{kangSelfAttentiveSequentialRecommendation2018,
  title = {Self-{{Attentive Sequential Recommendation}}},
  author = {Kang, Wang-Cheng and McAuley, Julian},
  year = {2018},
  month = aug,
  number = {arXiv:1808.09781},
  eprint = {1808.09781},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.09781},
  urldate = {2024-01-09},
  abstract = {Sequential dynamics are a key feature of many modern recommender systems, which seek to capture the `context' of users' activities on the basis of actions they have performed recently. To capture such patterns, two approaches have proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs). Markov Chains assume that a user's next action can be predicted on the basis of just their last (or last few) actions, while RNNs in principle allow for longer-term semantics to be uncovered. Generally speaking, MC-based methods perform best in extremely sparse datasets, where model parsimony is critical, while RNNs perform better in denser datasets where higher model complexity is affordable. The goal of our work is to balance these two goals, by proposing a self-attention based sequential model (SASRec) that allows us to capture long-term semantics (like an RNN), but, using an attention mechanism, makes its predictions based on relatively few actions (like an MC). At each time step, SASRec seeks to identify which items are `relevant' from a user's action history, and use them to predict the next item. Extensive empirical studies show that our method outperforms various state-of-the-art sequential models (including MC/CNN/RNN-based approaches) on both sparse and dense datasets. Moreover, the model is an order of magnitude more efficient than comparable CNN/RNN-based models. Visualizations on attention weights also show how our model adaptively handles datasets with various density, and uncovers meaningful patterns in activity sequences.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/I516998/Zotero/storage/N3B5CKQ9/Kang and McAuley - 2018 - Self-Attentive Sequential Recommendation.pdf;/Users/I516998/Zotero/storage/2HLJMQ43/1808.html}
}

@misc{LinkedInLearningMit,
  title = {{LinkedIn Learning mit Lynda: Onlinekurse aus dem Business-, Technik- und Kreativbereich}},
  shorttitle = {{LinkedIn Learning mit Lynda}},
  journal = {LinkedIn Learning mit Lynda: Onlinekurse aus dem Business-, Technik- und Kreativbereich},
  urldate = {2023-11-03},
  abstract = {Erreichen Sie Ihre pers{\"o}nlichen und beruflichen Ziele mit den passenden Weiterbildungsangeboten. Entscheiden Sie sich f{\"u}r LinkedIn Learning und nutzen Sie zahlreiche Kurse auf unserer Lernplattform. Lynda ist jetzt LinkedIn Learning.},
  howpublished = {https://de.linkedin.com/learning/},
  langid = {ngerman},
  file = {/Users/I516998/Zotero/storage/SQJCFMPW/learning.html}
}

@misc{MovieLens1MDataset2015,
  title = {{{MovieLens 1M Dataset}}},
  year = {2015},
  month = sep,
  journal = {GroupLens},
  urldate = {2024-01-09},
  abstract = {MovieLens 1M movie ratings. Stable benchmark dataset. 1 million ratings from 6000 users on 4000 movies. Released 2/2003. README.txt ml-1m.zip (size: 6 MB, checksum) Permalink:},
  howpublished = {https://grouplens.org/datasets/movielens/1m/},
  langid = {english},
  file = {/Users/I516998/Zotero/storage/BQNJHGMB/1m.html}
}

@article{radfordImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/Users/I516998/Zotero/storage/7YGQ5F4D/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@book{rubensonAdultLearningEducation2011,
  title = {Adult {{Learning}} and {{Education}}},
  author = {Rubenson, Kjell},
  year = {2011},
  month = feb,
  publisher = {{Academic Press}},
  abstract = {As individuals and societies try to respond to fundamental economic and social transformation, the field of adult learning and education is rapidly getting increased attention and new topics for research on adult learning have emerged. This collection of articles from the International Encyclopedia of Education 3e offers practitioners and researchers in the area of adult learning and education a comprehensive summary of main developments in the field. The 45 articles provide insight into the historical development of the field, its conceptual controversies, domains and provision, perspectives on adult learning, instruction and program planning, outcomes, relationship to economy and society and its status as a field of scholarly study and practice.  Saves researchers time in summarizing in one place what is otherwise an interdisciplinary field in cognitive psychology, personality, sociology, and education Level of presentation focuses on critical research, leaving out the extraneous and focusing on need-to-know information Contains contributions from top international researchers in the field Makes MRW content affordable to individual researchers},
  googlebooks = {wpsuGE5UQG8C},
  isbn = {978-0-12-381489-0},
  langid = {english},
  keywords = {Education / Adult \& Continuing Education}
}

@misc{StartseiteMoodleOrg,
  title = {{Startseite | Moodle.org}},
  urldate = {2024-01-05},
  abstract = {Moodle is a Learning Platform or course management system (CMS) - a free Open Source software package designed to help educators create effective online courses based on sound pedagogical principles. You can download and use it on any computer you have handy (including webhosts), yet it can scale from a single-teacher site to a 200,000-student University. Moodle has a large and diverse user community with over 100,000 sites registered worldwide speaking over 140 languages in every country there is.},
  howpublished = {https://moodle.org/},
  langid = {ngerman},
  file = {/Users/I516998/Zotero/storage/THG6EFDP/moodle.org.html}
}

@misc{suarezInformationRetrievalWeb2022,
  title = {Information {{Retrieval}} \& {{Web Search}}},
  shorttitle = {{{IR}} \& {{WS}}},
  author = {Suarez, Dr. Pedro Ortiz and Glava{\v s}, Prof. Dr. Goran and Ponzetto, Prof. Dr. Simone Paolo},
  year = {2022},
  month = sep,
  langid = {english}
}

@misc{sunBERT4RecSequentialRecommendation2019,
  title = {{{BERT4Rec}}: {{Sequential Recommendation}} with {{Bidirectional Encoder Representations}} from {{Transformer}}},
  shorttitle = {{{BERT4Rec}}},
  author = {Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng},
  year = {2019},
  month = aug,
  number = {arXiv:1904.06690},
  eprint = {1904.06690},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.06690},
  urldate = {2023-11-02},
  abstract = {Modeling users' dynamic and evolving preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks (e.g., Recurrent Neural Network) to encode users' historical interactions from left to right into hidden representations for making recommendations. Although these methods achieve satisfactory results, they often assume a rigidly ordered sequence which is not always practical. We argue that such left-to-right unidirectional architectures restrict the power of the historical sequence representations. For this purpose, we introduce a Bidirectional Encoder Representations from Transformers for sequential Recommendation (BERT4Rec). However, jointly conditioning on both left and right context in deep bidirectional model would make the training become trivial since each item can indirectly "see the target item". To address this problem, we train the bidirectional model using the Cloze task, predicting the masked items in the sequence by jointly conditioning on their left and right context. Comparing with predicting the next item at each position in a sequence, the Cloze task can produce more samples to train a more powerful bidirectional model. Extensive experiments on four benchmark datasets show that our model outperforms various state-of-the-art sequential models consistently.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/I516998/Zotero/storage/LUYIPKBX/Sun et al. - 2019 - BERT4Rec Sequential Recommendation with Bidirecti.pdf;/Users/I516998/Zotero/storage/FCC748IB/1904.html}
}

@misc{tangPersonalizedTopNSequential2018,
  title = {Personalized {{Top-N Sequential Recommendation}} via {{Convolutional Sequence Embedding}}},
  author = {Tang, Jiaxi and Wang, Ke},
  year = {2018},
  month = sep,
  number = {arXiv:1809.07426},
  eprint = {1809.07426},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1809.07426},
  urldate = {2024-01-09},
  abstract = {Top-\$N\$ sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-\$N\$ ranked items that a user will likely interact in a `near future'. The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. In this paper, we propose a Convolutional Sequence Embedding Recommendation Model ({\textbackslash}emph\{Caser\}) as a solution to address this requirement. The idea is to embed a sequence of recent items into an `image' in the time and latent spaces and learn sequential patterns as local features of the image using convolutional filters. This approach provides a unified and flexible network structure for capturing both general preferences and sequential patterns. The experiments on public datasets demonstrated that Caser consistently outperforms state-of-the-art sequential recommendation methods on a variety of common evaluation metrics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/I516998/Zotero/storage/3SRV7HMD/Tang and Wang - 2018 - Personalized Top-N Sequential Recommendation via C.pdf;/Users/I516998/Zotero/storage/H68HN67R/1809.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-01-12},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/I516998/Zotero/storage/B8HZNEVY/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/I516998/Zotero/storage/DPBGKNR6/1706.html}
}

@inproceedings{wanFineGrainedSpoilerDetection2019,
  title = {Fine-{{Grained Spoiler Detection}} from {{Large-Scale Review Corpora}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wan, Mengting and Misra, Rishabh and Nakashole, Ndapa and McAuley, Julian},
  year = {2019},
  pages = {2605--2610},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1248},
  urldate = {2024-01-09},
  abstract = {This paper presents computational approaches for automatically detecting critical plot twists in reviews of media products. First, we created a large-scale book review dataset that includes fine-grained spoiler annotations at the sentence-level, as well as book and (anonymized) user information. Second, we carefully analyzed this dataset, and found that: spoiler language tends to be book-specific; spoiler distributions vary greatly across books and review authors; and spoiler sentences tend to jointly appear in the latter part of reviews. Third, inspired by these findings, we developed an end-to-end neural network architecture to detect spoiler sentences in review corpora. Quantitative and qualitative results demonstrate that the proposed method substantially outperforms existing baselines.},
  langid = {english},
  file = {/Users/I516998/Zotero/storage/QF6PJMG6/Wan et al. - 2019 - Fine-Grained Spoiler Detection from Large-Scale Re.pdf}
}

@article{wangSequentialRecommenderSystems2019,
  title = {Sequential {{Recommender Systems}}: {{Challenges}}, {{Progress}} and {{Prospects}}},
  shorttitle = {Sequential {{Recommender Systems}}},
  author = {Wang, Shoujin and Hu, Liang and Wang, Yan and Cao, Longbing and Sheng, Quan Z. and Orgun, Mehmet},
  year = {2019},
  pages = {6332--6338},
  urldate = {2023-11-02},
  abstract = {Electronic proceedings of IJCAI 2019},
  file = {/Users/I516998/Zotero/storage/L4I2KE8J/883.html}
}

@misc{YelpDataset,
  title = {Yelp {{Dataset}}},
  urldate = {2024-01-09},
  howpublished = {https://www.yelp.com/dataset}
}

@inproceedings{yuDynamicRecurrentModel2016,
  title = {A {{Dynamic Recurrent Model}} for {{Next Basket Recommendation}}},
  booktitle = {Proceedings of the 39th {{International ACM SIGIR}} Conference on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang and Tan, Tieniu},
  year = {2016},
  month = jul,
  series = {{{SIGIR}} '16},
  pages = {729--732},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2911451.2914683},
  urldate = {2023-11-02},
  abstract = {Next basket recommendation becomes an increasing concern. Most conventional models explore either sequential transaction features or general interests of users. Further, some works treat users' general interests and sequential behaviors as two totally divided matters, and then combine them in some way for next basket recommendation. Moreover, the state-of-the-art models are based on the assumption of Markov Chains (MC), which only capture local sequential features between two adjacent baskets. In this work, we propose a novel model, Dynamic REcurrent bAsket Model (DREAM), based on Recurrent Neural Network (RNN). DREAM not only learns a dynamic representation of a user but also captures global sequential features among baskets. The dynamic representation of a specific user can reveal user's dynamic interests at different time, and the global sequential features reflect interactions of all baskets of the user over time. Experiment results on two public datasets indicate that DREAM is more effective than the state-of-the-art models for next basket recommendation.},
  isbn = {978-1-4503-4069-4},
  keywords = {next basket recommendation,recurrent neural network}
}
